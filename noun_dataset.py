# Noun embedding dataset

# Imports
from __future__ import annotations
import os
import re
import math
import json
import mmap
import string
import random
import hashlib
import itertools
import functools
import contextlib
import dataclasses
from typing import Union, Iterable, Optional, BinaryIO, ContextManager, Any
import tqdm
import numpy as np
import torch.utils.data
from logger import log
import utils
import embedders
import embedding_dataset

# Prompt template class
@dataclasses.dataclass(frozen=True)
class PromptTemplate:
	template: str         # Prompt template text including a single occurrence of {noun}
	need_article: bool    # Whether the {noun} field requires an article
	need_hyp_comma: bool  # Whether the {noun} field is not followed by punctuation and thus needs a closing comma separator when using hypernym templates
	freq: int             # Frequency of the prompt template

# Hypernym template class
@dataclasses.dataclass(frozen=True)
class HypernymTemplate:
	template: str        # Hypernym template text including a single occurrence of {target}
	template_comma: str  # Same as template, but with a trailing comma
	need_article: bool   # Whether the template text includes an {article} field
	freq_dist: tuple[tuple[int, ...], ...]  # Frequency of this hypernym template for each hypernym index, as a function of the number of hypernym indices

# Unique sample class
@dataclasses.dataclass(frozen=True)
class UniqueSample:
	fsid: int              # First frequenced sample ID
	usid: int              # Unique sample ID
	freq: int              # Frequency of the sample
	vocab: dict[str, Any]  # Vocabulary data
	noun: str              # Inserted noun
	text: str              # Sample text
	target: str            # Target noun

# Noun dataset cache class
@dataclasses.dataclass(frozen=True)
class Cache:
	file: Optional[BinaryIO]    # Opened binary cache file object
	mmap: Optional[mmap.mmap]   # Memory map of the cache file
	view: Optional[memoryview]  # Bytes-like view of the memory-mapped data
	embed_dtype: torch.dtype    # PyTorch tensor data type for embedding vectors
	target_dtype: torch.dtype   # PyTorch tensor data type for target noun tokenizations
	mask_dtype: torch.dtype     # PyTorch tensor data type for target noun tokenization padding masks
	embed_dim: int              # Dimension of embedding vectors
	target_dim: int             # Dimension of target noun tokenizations
	mask_dim: int               # Dimension of target noun tokenization padding masks (0 = No mask)
	embed_bytes: int            # Number of bytes required for an embedding vector
	target_bytes: int           # Number of bytes required for target noun tokenizations
	mask_bytes: int             # Number of bytes required for target noun tokenization padding masks
	sample_bytes: int           # Total number of bytes per USID sample (sum of embed_bytes and target_bytes and mask_bytes)

# Noun dataset class
class NounDataset(embedding_dataset.EmbeddingDataset):

	#
	# Construction
	#

	def __init__(
		self,
		embedder: embedders.Embedder,                    # Embedder that is to be used in the context of the noun dataset (any existing target_config is ignored for now)
		vocab_path: str,                                 # String path to the JSON file containing the noun vocabulary
		prompt_path: str,                                # String path to the JSON file containing the prompt templates
		prompt_collection: Union[str, Iterable[str]],    # Prompt template collection(s) to use (str = Can be |-separated, or iterable of str)
		hypernym_collection: Union[str, Iterable[str]],  # Hypernym template collection(s) to use (str = Can be |-separated, or iterable of str)
		vocab_thres: int = 0,                            # Eliminate all entries from the vocabulary with total singular plus plural frequency of less or equal to this value (hypernyms are not filtered out and can as such be target nouns outside of the filtered vocab)
		cache_dir: Optional[str] = None,                 # Directory to use to establish and/or use a cache of embedding vectors and token IDs (the ensure_cache() method must be called if this is not None, in order to actually generate/load the cache)
		force_recache: bool = False,                     # Whether to force any existing cache to be cleared and recached
		check_consistent: bool = False,                  # Whether to check dataset consistency, in particular whether the dataset generated by indexed access is identical to the dataset generated by looping
		check_print: int = 0,                            # How many USIDs to print for manual dataset checking (0 = None)
		use_targets: Optional[bool] = None,              # Whether target information should be returned from the dataset (None = Auto)
	):

		self.vocab_path = vocab_path
		with open(self.vocab_path, 'r') as file:
			self.vocab_json = json.load(file)
		log.info(f"Loaded {len(self.vocab_json)} target nouns from vocab JSON: {self.vocab_path}")

		self.vocab_by_id = {}
		for vocab in self.vocab_json:
			vocab_id = vocab['id']
			if not isinstance(vocab_id, int) or vocab_id < 0:
				raise ValueError(f"Vocab JSON has invalid vocab ID: {vocab_id}")
			if vocab_id in self.vocab_by_id:
				raise ValueError(f"Vocab JSON has duplicate vocab ID: {vocab_id}")
			self.vocab_by_id[vocab_id] = vocab  # noqa
			if not vocab['target_noun']:
				raise ValueError(f"Vocab ID {vocab_id} has empty target noun")
			singulars = vocab['singulars']
			plurals = vocab['plurals']
			singulars_freq = vocab['singulars_freq']
			plurals_freq = vocab['plurals_freq']
			if len(singulars) != len(singulars_freq):
				raise ValueError(f"Vocab ID {vocab_id} has non-matching number of singulars to freqs")
			if len(plurals) != len(plurals_freq):
				raise ValueError(f"Vocab ID {vocab_id} has non-matching number of plurals to freqs")
			if len(singulars) + len(plurals) < 1:
				raise ValueError(f"Vocab ID {vocab_id} has neither singulars nor plurals")
			if any(not isinstance(freq, int) or freq < 1 for freq in itertools.chain(singulars_freq, plurals_freq)):
				raise ValueError(f"Vocab ID {vocab_id} has invalid frequencies (must be positive integers)")
			if any(noun != ' '.join(noun.split()) for noun in itertools.chain((vocab['target_noun'], vocab['pretty_noun']), singulars, plurals)):
				raise ValueError(f"Vocab ID {vocab_id} has a badly whitespaced noun")
			vocab['singulars_freq_sum'] = sum(singulars_freq)
			vocab['plurals_freq_sum'] = sum(plurals_freq)
			vocab['singulars_id'] = tuple(noun_id for noun_id, freq in enumerate(singulars_freq) for _ in range(freq))
			vocab['plurals_id'] = tuple(noun_id for noun_id, freq in enumerate(plurals_freq) for _ in range(freq))

		self.vocab_thres = vocab_thres
		if self.vocab_thres > 0:
			self.vocab_json = [vocab for vocab in self.vocab_json if vocab['singulars_freq_sum'] + vocab['plurals_freq_sum'] > self.vocab_thres]
			log.info(f"Filtered vocab down to {len(self.vocab_json)} target nouns using a frequency threshold of >{self.vocab_thres}")
		self.target_nouns: tuple[str, ...] = tuple(vocab['target_noun'] for vocab in self.vocab_json)  # noqa
		if len(set(self.target_nouns)) != len(self.target_nouns):
			raise ValueError("Target nouns contains duplicates")  # Note: The base class __init__ does a better time of identifying which target nouns are repeated how often

		max_hypernyms = 0
		for vocab in self.vocab_json:
			hypernyms = vocab['hypernyms']
			hypernyms_set = set(hypernyms)
			if len(hypernyms_set) < len(hypernyms):
				raise ValueError(f"Vocab ID {vocab['id']} has duplicate hypernym(s)")
			max_hypernyms = max(max_hypernyms, len(hypernyms))
			if any(hypernym_id not in self.vocab_by_id for hypernym_id in hypernyms):
				raise ValueError(f"Vocab ID {vocab['id']} has invalid hypernym(s)")
			vocab['hypernym_targets'] = tuple(self.vocab_by_id[hypernym_id]['target_noun'] for hypernym_id in hypernyms)  # noqa
		log.info(f"Maximum number of hypernyms for a target noun is {max_hypernyms}")

		log.info(f"Loaded vocabulary after preprocessing is {utils.get_size_mb(self.vocab_json):.1f}MiB")

		self.prompt_path = prompt_path
		with open(self.prompt_path, 'r') as file:
			prompt_json = json.load(file)
		log.info(f"Loaded prompt templates from JSON: {self.prompt_path}")
		prompts_json = prompt_json['prompts']
		loaded_prompts_str = ', '.join(f'{coll} \u00D7 {len(items)}' for coll, items in prompts_json.items())
		log.info(f"Loaded prompt collections: {loaded_prompts_str}")
		hypernyms_json = prompt_json['hypernyms']
		loaded_hypernyms_str = ', '.join(f'{coll} \u00D7 {len(items)}' for coll, items in hypernyms_json.items())
		log.info(f"Loaded hypernym collections: {loaded_hypernyms_str}")

		prompt_freq_map = {}
		formatter = string.Formatter()
		self.prompt_collection = self._parse_collection(collection=prompt_collection, allowed=prompts_json)
		for collection in self.prompt_collection:
			for freq, prompt_template in prompts_json[collection]:
				if not isinstance(freq, int) or freq < 0:
					raise ValueError(f"Prompt frequency must be a positive integer: {freq}")
				if freq >= 1:
					prompt_freq_map[prompt_template] = prompt_freq_map.get(prompt_template, 0) + freq

		singular_prompts = []
		plural_prompts = []
		for prompt_template, freq in prompt_freq_map.items():
			prompt_template = ' '.join(prompt_template.split())
			if not prompt_template:
				raise ValueError(f"Empty prompt template")
			if prompt_template[-1] != '.':
				prompt_template += '.'
			prompt_template_parts = tuple(formatter.parse(prompt_template))
			if any(format_spec or conversion for _, _, format_spec, conversion in prompt_template_parts):
				raise ValueError(f"Prompt template fields cannot have format spec or conversion: {prompt_template}")
			prompt_template_fields = set(field_name for _, field_name, _, _ in prompt_template_parts)
			if len(prompt_template_fields) != len(prompt_template_parts):
				raise ValueError(f"Prompt template cannot specify a field multiple times: {prompt_template}")
			prompt_template_fields.discard(None)
			if len(prompt_template_fields) != 1:
				raise ValueError(f"Must have exactly one prompt template field: {prompt_template}")
			prompt_type = prompt_template_fields.pop()
			prompt_type_field = f'{{{prompt_type}}}'
			need_hyp_comma = prompt_template[prompt_template.rfind(prompt_type_field) + len(prompt_type_field)] not in ('.', ',', '?', '!', ':', ';')  # Note: This is never an index error because the last character is always '.' and thus never '}'
			if prompt_type != 'noun':
				prompt_template = prompt_template.replace(prompt_type_field, '{noun}')
			if need_article := prompt_type.startswith('article_'):
				prompt_type = prompt_type[8:]
			if prompt_type == 'singular':
				singular_prompts.append(PromptTemplate(template=prompt_template, freq=freq, need_article=need_article, need_hyp_comma=need_hyp_comma))
			elif prompt_type == 'plural':
				plural_prompts.append(PromptTemplate(template=prompt_template, freq=freq, need_article=False, need_hyp_comma=need_hyp_comma))
			elif prompt_type == 'noun':
				singular_prompts.append(PromptTemplate(template=prompt_template, freq=freq, need_article=need_article, need_hyp_comma=need_hyp_comma))
				plural_prompts.append(PromptTemplate(template=prompt_template, freq=freq, need_article=False, need_hyp_comma=need_hyp_comma))
			else:
				raise ValueError(f"Unrecognised prompt template field {{{prompt_type}}}: {prompt_template}")
		self.singular_prompts = tuple(singular_prompts)
		self.plural_prompts = tuple(plural_prompts)
		num_singular_prompts = len(self.singular_prompts)
		num_plural_prompts = len(self.plural_prompts)
		log.info(f"Compiled {num_singular_prompts} singular and {num_plural_prompts} plural prompt templates from collections: {' | '.join(self.prompt_collection)}")

		hypernym_freq_map = {}
		self.hypernym_collection = self._parse_collection(collection=hypernym_collection, allowed=hypernyms_json)
		for collection in self.hypernym_collection:
			for freq, hypernym_template in hypernyms_json[collection]:
				if not isinstance(freq, int) or freq < 0:
					raise ValueError(f"Hypernym frequency must be a positive integer: {freq}")
				if freq >= 1:
					hypernym_freq_map[hypernym_template] = hypernym_freq_map.get(hypernym_template, 0) + freq
		if None not in hypernym_freq_map:
			raise ValueError("One of the hypernym templates must be null/None")
		no_hypernym_freq = hypernym_freq_map.pop(None)
		num_hypernym_templates = len(hypernym_freq_map)
		hypernym_template_freq_map = {0: tuple(() for _ in range(num_hypernym_templates))}
		for num_hypernyms in range(1, max_hypernyms + 1):
			hypernym_dist = tuple([0] * num_hypernyms for _ in range(num_hypernym_templates))
			i_hypernym = 0
			for i_template, template_freq in enumerate(hypernym_freq_map.values()):
				for _ in range(template_freq):
					hypernym_dist[i_template][i_hypernym] += 1
					i_hypernym = (i_hypernym + 1) % num_hypernyms
			hypernym_template_freq_map[num_hypernyms] = tuple(tuple(dist) for dist in hypernym_dist)
		self.no_hypernym_freqs = (no_hypernym_freq + sum(hypernym_freq_map.values()),) + (no_hypernym_freq,) * max_hypernyms

		hypernym_prompts = []
		for i, (hypernym_template, freq) in enumerate(hypernym_freq_map.items()):
			hypernym_template = ' '.join(hypernym_template.split())
			if not hypernym_template:
				raise ValueError(f"Empty hypernym template")
			hypernym_template_parts = tuple(formatter.parse(hypernym_template))
			if any(format_spec or conversion for _, _, format_spec, conversion in hypernym_template_parts):
				raise ValueError(f"Hypernym template fields cannot have format spec or conversion: {hypernym_template}")
			hypernym_template_fields = set(field_name for _, field_name, _, _ in hypernym_template_parts)
			if len(hypernym_template_fields) != len(hypernym_template_parts):
				raise ValueError(f"Hypernym template cannot specify a field multiple times: {hypernym_template}")
			hypernym_template_fields.discard(None)
			if len(hypernym_template_fields) != 1:
				raise ValueError(f"Must have exactly one hypernym template field: {hypernym_template}")
			hypernym_type = hypernym_template_fields.pop()
			if need_article := hypernym_type.startswith('article_'):
				hypernym_type = hypernym_type[8:]
				hypernym_template = hypernym_template.replace('{article_', '{')
			if hypernym_type == 'target':
				hypernym_prompts.append(HypernymTemplate(template=hypernym_template, template_comma=hypernym_template + ',', need_article=need_article, freq_dist=tuple(dist[i] for dist in hypernym_template_freq_map.values())))
			else:
				raise ValueError(f"Unrecognised hypernym template field {{{hypernym_type}}}: {hypernym_template}")
		self.hypernym_prompts = tuple(hypernym_prompts)
		log.info(f"Compiled {len(self.hypernym_prompts)} hypernym prompt templates from collections: {' | '.join(self.hypernym_collection)}")

		log.info("Calculating required dataset indices and mappings...")
		hypernym_block_map = []
		unique_hypernym_map = []
		unique_hypernyms = []
		for num_hypernyms in range(max_hypernyms + 1):
			block_map: list = [(None, None, 0, self.no_hypernym_freqs[num_hypernyms], repetition) for repetition in range(self.no_hypernym_freqs[num_hypernyms])]
			unique_map: list = [(None, None)]
			for hypernym in self.hypernym_prompts:
				for hypernym_id, freq in enumerate(hypernym.freq_dist[num_hypernyms]):
					if freq > 0:
						for repetition in range(freq):
							block_map.append((hypernym, hypernym_id, len(unique_map), freq, repetition))
						unique_map.append((hypernym, hypernym_id))
			hypernym_block_map.append(tuple(block_map))
			unique_hypernym_map.append(tuple(unique_map))
			unique_hypernyms.append(len(unique_map))
		self.hypernym_block_map = tuple(hypernym_block_map)
		self.unique_hypernym_map = tuple(unique_hypernym_map)
		self.unique_hypernyms = tuple(unique_hypernyms)
		total_freq_singular_prompts = sum(prompt.freq for prompt in self.singular_prompts)
		total_freq_plural_prompts = sum(prompt.freq for prompt in self.plural_prompts)
		self.total_freq_hypernyms = self.no_hypernym_freqs[0]
		total_freq_singular = total_freq_singular_prompts * self.total_freq_hypernyms
		total_freq_plural = total_freq_plural_prompts * self.total_freq_hypernyms
		self.num_tids = len(self.vocab_json)
		self.target_fsid_map = np.fromiter(itertools.accumulate((freq for vocab in self.vocab_json for freq in (total_freq_singular * vocab['singulars_freq_sum'], total_freq_plural * vocab['plurals_freq_sum'])), initial=0), dtype=np.int64, count=2 * self.num_tids + 1)
		self.target_usid_map = np.fromiter(itertools.accumulate((self.unique_hypernyms[len(vocab['hypernyms'])] * freq for vocab in self.vocab_json for freq in (num_singular_prompts * len(vocab['singulars']), num_plural_prompts * len(vocab['plurals']))), initial=0), dtype=np.int64, count=2 * self.num_tids + 1)
		self.num_fsids = int(self.target_fsid_map[-1])
		self.num_usids = int(self.target_usid_map[-1])
		self.prompt_block_map_singular = tuple((prompt_id, prompt, repetition) for prompt_id, prompt in enumerate(self.singular_prompts) for repetition in range(prompt.freq))
		self.prompt_block_map_plural = tuple((prompt_id, prompt, repetition) for prompt_id, prompt in enumerate(self.plural_prompts) for repetition in range(prompt.freq))
		log.info(f"Finished calculating indices and mappings for {self.num_tids} TIDs, {self.num_fsids} FSIDs, and {self.num_usids} USIDs")

		log.info(f"Total memory use for noun dataset is {utils.get_size_mb(self):.1f}MiB")

		if cache_dir is None:
			self.cache_dir = None
			self.use_cache = False
			self.recache = False
			log.info("Noun dataset is configured to return text strings")
		else:
			self.cache_dir = os.path.abspath(os.path.expanduser(cache_dir))
			self.use_cache = True
			self.recache = force_recache
			log.info(f"Noun dataset is configured to directly return text embeddings and token IDs with use of a {'regenerated ' if self.recache else ''}cache")
		self.cache = None

		super().__init__(
			embedder=embedder,
			nominal_data_config=embedding_dataset.DataConfig(
				use_weights=False,
				unit_weights=True,
				multi_target=False,
				multi_first=False,
				full_targets=True,
				fixed_multi_length=True,
				multi_length=1,
			),
			strict_data_config_fields={'multi_length'},
			num_items=self.num_fsids,
			targets=self.target_nouns,
			use_targets=True if use_targets is None else use_targets,
		)

		self.check_consistent = check_consistent
		self.check(max_print_usid=check_print)

	@classmethod
	def _parse_collection(cls, collection: Union[str, Iterable[str]], allowed: Iterable[str]) -> tuple[str, ...]:
		if isinstance(collection, str):
			collection = tuple(coll.strip() for coll in collection.split('|'))
		elif not isinstance(collection, tuple):
			collection = tuple(collection)
		if not all(isinstance(coll, str) for coll in collection):
			raise TypeError(f"Collection specifier should be str or Iterable[str]: {collection}")
		if any(coll not in allowed for coll in collection):
			raise ValueError(f"Collection specifier contains invalid str values: {collection}")
		return collection

	def check(self, max_print_usid=0):
		# max_print_usid = Maximum USID to print for manual dataset checking
		if self.check_consistent or max_print_usid > 0:
			log.info("Checking and/or printing noun dataset...")
			continued = False
			seen_text = set()
			for sample in self.unique_sample(progress_bar_desc='Checking dataset'):
				if sample.usid < max_print_usid:
					print(f"{sample.fsid}:{sample.usid}: {sample.freq:2d} \u00D7 {sample.text}")
					if sample.text in seen_text:
						if sample.noun not in sample.vocab['singulars'] or sample.noun not in sample.vocab['plurals']:
							raise RuntimeError(f"Multiple disjoint occurrences of text: {sample.text}")
					seen_text.add(sample.text)
				elif not continued:
					if max_print_usid > 0:
						print("Continued...")
					continued = True
					seen_text = None
					if not self.check_consistent:
						return
			log.info("Successfully finished checking and/or printing noun dataset")

	#
	# Dataset interface
	#

	def loaded(self) -> ContextManager:
		if self.translation is not None and self.translation != self.embedder.target_config:
			raise RuntimeError("Noun dataset does not support target configuration translation")
		if self.use_cache:
			return self.cached()
		else:
			return self.embedder.inference_model()

	def __getitem__(self, index) -> Union[tuple[str, Optional[str], None, None], tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor], None]]:
		if self.use_cache:
			return self.get_cached_sample(usid=self.get_sample_usid(index))
		else:
			text, target_noun = self.get_sample_text(fsid=index)
			return text, target_noun if self.use_targets else None, None, None

	def get_sample_details(self, fsid=None, usid=None) -> tuple[int, dict[str, Any], str, bool, list[str], PromptTemplate, Optional[HypernymTemplate], Optional[str], int]:
		# Note: At least one argument must be provided, and if both are provided the USID is used

		if usid is not None:

			if usid < 0 or usid >= self.num_usids:
				raise IndexError("USID out of range")
			target_usid_map_index = int(self.target_usid_map.searchsorted(usid, side='right')) - 1
			is_singular = not (target_usid_map_index & 1)
			target_usid = usid - int(self.target_usid_map[target_usid_map_index])

			vocab = self.vocab_json[target_usid_map_index >> 1]
			target_noun = vocab['target_noun']
			nouns = vocab['singulars' if is_singular else 'plurals']
			hypernym_targets = vocab['hypernym_targets']
			num_hypernyms = len(hypernym_targets)

			target_usid, noun_id = divmod(target_usid, len(nouns))
			prompt_id, unique_hypernym_id = divmod(target_usid, self.unique_hypernyms[num_hypernyms])
			prompt = self.singular_prompts[prompt_id] if is_singular else self.plural_prompts[prompt_id]
			hypernym, hypernym_id = self.unique_hypernym_map[num_hypernyms][unique_hypernym_id]
			hypernym_target = hypernym_targets[hypernym_id] if hypernym_id is not None else None

		else:

			if fsid < 0 or fsid >= self.num_fsids:
				raise IndexError("FSID out of range")
			target_fsid_map_index = int(self.target_fsid_map.searchsorted(fsid, side='right')) - 1
			is_singular = not (target_fsid_map_index & 1)
			target_fsid = fsid - int(self.target_fsid_map[target_fsid_map_index])

			vocab = self.vocab_json[target_fsid_map_index >> 1]
			target_noun = vocab['target_noun']
			nouns_freq_sum = vocab['singulars_freq_sum' if is_singular else 'plurals_freq_sum']
			hypernym_targets = vocab['hypernym_targets']
			num_hypernyms = len(hypernym_targets)

			prompt_block_size = nouns_freq_sum * self.total_freq_hypernyms
			prompt_block_id, prompt_block_offset = divmod(target_fsid, prompt_block_size)
			prompt_id, prompt, prompt_repetition = (self.prompt_block_map_singular if is_singular else self.prompt_block_map_plural)[prompt_block_id]
			prompt_block_offset += prompt_repetition * prompt_block_size

			hypernym_block_size = prompt.freq * nouns_freq_sum
			hypernym_block_id, hypernym_block_offset = divmod(prompt_block_offset, hypernym_block_size)
			hypernym, hypernym_id, unique_hypernym_id, hypernym_freq, hypernym_repetition = self.hypernym_block_map[num_hypernyms][hypernym_block_id]
			hypernym_block_offset += hypernym_repetition * hypernym_block_size
			hypernym_target = hypernym_targets[hypernym_id] if hypernym_id is not None else None

			noun_block_size = prompt.freq * hypernym_freq
			noun_block_id = hypernym_block_offset // noun_block_size
			noun_id = vocab['singulars_id' if is_singular else 'plurals_id'][noun_block_id]

			nouns = vocab['singulars' if is_singular else 'plurals']
			usid = int(self.target_usid_map[target_fsid_map_index]) + noun_id + len(nouns) * (unique_hypernym_id + self.unique_hypernyms[num_hypernyms] * prompt_id)

		return usid, vocab, target_noun, is_singular, nouns, prompt, hypernym, hypernym_target, noun_id

	def get_sample_usid(self, fsid) -> int:
		return self.get_sample_details(fsid)[0]

	def get_sample_text(self, fsid=None, usid=None, details=None) -> tuple[str, str]:
		if details is None:
			details = self.get_sample_details(fsid=fsid, usid=usid)
		usid, vocab, target_noun, is_singular, nouns, prompt, hypernym, hypernym_target, noun_id = details
		noun = nouns[noun_id]
		if hypernym is None:
			if prompt.need_article:
				noun = self.make_indefinite(noun)
		else:
			hypernym_template = hypernym.template_comma if prompt.need_hyp_comma else hypernym.template
			hypernym_suffix = hypernym_template.format(target=self.make_indefinite(hypernym_target) if hypernym.need_article else hypernym_target)
			noun = self.make_indefinite(noun, suffix=hypernym_suffix) if prompt.need_article else noun + hypernym_suffix
		text = prompt.template.format(noun=noun)
		return text, target_noun

	def get_sample_usid_text(self, fsid) -> tuple[int, str, str]:
		details = self.get_sample_details(fsid=fsid)
		return details[0], *self.get_sample_text(details=details)  # noqa

	def unique_sample(self, progress_bar_desc=None):
		# progress_bar_desc = Description to use for a progress bar (None = No progress bar)
		# Sequential generator for every unique sample in the dataset (len is self.num_usids)

		fsid = 0  # FSID = Frequenced sample ID = Sample ID where every text repetition counts as a new ID (indexes the __getitem__-based dataset)
		usid = 0  # USID = Unique sample ID = Sample ID where every unique text is only counted once (indexes the sample cache)

		with tqdm.tqdm(desc=progress_bar_desc, total=self.num_usids, unit='id', unit_scale=True, dynamic_ncols=True, smoothing=0.08, disable=progress_bar_desc is None) as progress_bar:
			for vocab in self.vocab_json:
				target = vocab['target_noun']
				hypernym_targets = vocab['hypernym_targets']
				num_hypernym_targets = len(hypernym_targets)
				for prompts, nouns, freqs in ((self.singular_prompts, vocab['singulars'], vocab['singulars_freq']), (self.plural_prompts, vocab['plurals'], vocab['plurals_freq'])):
					for prompt in prompts:

						hypernym_data = [(None, self.no_hypernym_freqs[num_hypernym_targets])]
						for hypernym in self.hypernym_prompts:
							hypernym_template = hypernym.template_comma if prompt.need_hyp_comma else hypernym.template
							for hypernym_target, hypernym_target_freq in zip(hypernym_targets, hypernym.freq_dist[num_hypernym_targets]):
								if hypernym_target_freq > 0:
									hypernym_data.append((hypernym_template.format(target=self.make_indefinite(hypernym_target) if hypernym.need_article else hypernym_target), hypernym_target_freq))

						for hypernym_suffix, hypernym_target_freq in hypernym_data:
							for noun, freq in zip(nouns, freqs):

								if prompt.need_article:
									noun = self.make_indefinite(noun, suffix=hypernym_suffix)
								elif hypernym_suffix is not None:
									noun += hypernym_suffix
								text = prompt.template.format(noun=noun)
								total_freq = prompt.freq * hypernym_target_freq * freq

								if self.check_consistent:
									for check_fsid in range(fsid, fsid + total_freq):
										check_usid, check_text, check_target = self.get_sample_usid_text(check_fsid)
										if check_usid != usid or check_text != text or check_target != target:
											raise RuntimeError(f"Mismatch between dataset generator and indexed-get for FSID {check_fsid}: USID {usid} vs {check_usid}, Text '{text}' vs '{check_text}', Target '{target}' vs '{check_target}'")

								unique_sample = UniqueSample(fsid=fsid, usid=usid, freq=total_freq, vocab=vocab, noun=noun, text=text, target=target)
								yield unique_sample  # Note: If an exception occurs while yielded, a bizarre semi-unrelated exception may be displayed instead for some reason

								fsid += total_freq
								usid += 1
								progress_bar.update()

		if self.check_consistent and (fsid != self.num_fsids or usid != self.num_usids):
			raise RuntimeError(f"Dataset generator did not generate the correct number of samples: FSID {fsid} vs {self.num_fsids}, USID {usid} vs {self.num_usids}")

	class DataLoader(torch.utils.data.DataLoader):

		dataset: NounDataset

		def __init__(self, dataset: NounDataset, *args, patch_embed: bool, patch_device: Optional[torch.device], **kwargs):
			super().__init__(dataset, *args, **kwargs)
			self.patch_embed = patch_embed
			self.patch_device = patch_device

		def __iter__(self) -> Iter:
			# Note: We return a NounDataset.DataLoader.Iter instead of a torch.utils.data.dataloader._BaseDataLoaderIter (which the base class is annotated to return) but this is not an issue
			return self.Iter(loader=self, patch_embed=self.patch_embed, patch_device=self.patch_device)

		class Iter:

			def __init__(self, loader: NounDataset.DataLoader, patch_embed: bool, patch_device: Optional[torch.device]):
				self.loader = loader
				self.patch_embed = patch_embed
				self.patch_device = patch_device
				assert self.patch_device is None or self.patch_device.type != 'cpu'
				assert self.patch_embed or self.patch_device
				self.embedder = self.loader.dataset.embedder
				self.target_config = self.embedder.target_config
				self.data_config = self.loader.dataset.data_config
				self.raw_iter = super(NounDataset.DataLoader, self.loader).__iter__()

			def __iter__(self) -> NounDataset.DataLoader.Iter:
				return self

			def __next__(self) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:

				embed, target, mask, weight = next(self.raw_iter)

				if self.patch_embed:

					with self.embedder.inference_mode():
						embed = self.embedder.inference_text(text=embed)
					if target is not None:
						target, mask = self.embedder.tokenize_target(text=target)
						embed, target, mask, weight = NounDataset._apply_data_config(embed=embed, target=target, mask=mask, data_config=self.data_config)  # Note: __getitem__ always returns a weight of None so prior to applying the data config the weight can only be None and thus can be ignored

					if self.patch_device:
						if target is not None:
							target = target.pin_memory().to(device=self.patch_device, non_blocking=True)
						if mask is not None:
							mask = mask.pin_memory().to(device=self.patch_device, non_blocking=True)
						if weight is not None:
							weight = weight.pin_memory().to(device=self.patch_device, non_blocking=True)

				elif self.patch_device:

					embed = embed.to(device=self.patch_device, non_blocking=True)
					if target is not None:
						target = target.to(device=self.patch_device, non_blocking=True)
					if mask is not None:
						mask = mask.to(device=self.patch_device, non_blocking=True)
					if weight is not None:
						weight = weight.to(device=self.patch_device, non_blocking=True)

				return embed, target, mask, weight

	def create_loader(self, batch_size: int, num_workers: int, training: bool, device: torch.device, patch: bool = True) -> tuple[torch.utils.data.DataLoader, embedding_dataset.LoaderInfo]:

		if batch_size < 1:
			raise ValueError(f"Invalid batch size: {batch_size}")

		device_is_cpu = (device.type == 'cpu')
		collate_fn = functools.partial(self._collate_fn, target_config=self.embedder.target_config, data_config=self.data_config) if self.use_cache and self.use_targets else utils.default_collate
		loader_kwargs = dict(dataset=self, batch_size=batch_size, shuffle=training, num_workers=num_workers, collate_fn=collate_fn, pin_memory=not device_is_cpu and self.use_cache, drop_last=training)
		if patched := (patch and not (self.use_cache and device_is_cpu)):
			loader = self.DataLoader(**loader_kwargs, patch_embed=not self.use_cache, patch_device=None if device_is_cpu else device)
		else:
			loader = torch.utils.data.DataLoader(**loader_kwargs)

		available_samples = len(self)
		epoch_batches = len(loader)
		leftover_samples = available_samples % loader.batch_size
		incomplete_batch = not loader.drop_last and leftover_samples != 0
		batch_size_last = leftover_samples if incomplete_batch else 0

		loader_info = embedding_dataset.LoaderInfo(
			num_workers=loader.num_workers,
			prefetch_factor=0 if loader.prefetch_factor is None else loader.prefetch_factor,
			pin_memory=not (device_is_cpu or patched),
			on_device=device_is_cpu or patched,
			batch_size=loader.batch_size,
			batch_size_last=batch_size_last,
			complete_batches=epoch_batches - incomplete_batch,
			incomplete_batch=incomplete_batch,
			epoch_batches=epoch_batches,
			epoch_samples=available_samples if incomplete_batch else available_samples - leftover_samples,
			available_samples=available_samples,
		)

		return loader, loader_info

	@staticmethod
	def _collate_fn(batch: list[tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor], None]], target_config: embedders.TargetConfig, data_config: embedding_dataset.DataConfig) -> tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:
		# Collate a list of samples returned from __getitem__ into a batch and apply the required target and data configurations
		# This method assumes it will only ever be called with tensorized data that has targets available (but not necessarily masks or weights)

		embed, target, mask, _ = utils.default_collate(batch)  # Note: __getitem__ always returns a weight of None so prior to applying the data config the weight can only be None and thus can be ignored

		if not target_config.fixed_token_length and mask is not None:
			col_masked, col_index = mask.all(dim=0).max(dim=0)  # Note: This only finds the FIRST all-padding column, as it is strictly assumed that no non-all-padding column can happen after an all-padding column
			if col_masked:
				target = target[:, :col_index]
				mask = mask[:, :col_index]

		return NounDataset._apply_data_config(embed=embed, target=target, mask=mask, data_config=data_config)

	@staticmethod
	def _apply_data_config(embed: torch.Tensor, target: torch.Tensor, mask: Optional[torch.Tensor], data_config: embedding_dataset.DataConfig) -> tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:
		# Take a collated batch and apply the required target and data configurations
		# This method assumes it will only ever be called with tensorized data that has targets available (but not necessarily masks or weights)

		weight = torch.ones(size=(embed.shape[0],), dtype=embed.dtype) if data_config.use_weights else None

		if data_config.multi_target:
			dim = 1 - data_config.multi_first
			target = target.unsqueeze(dim=dim)
			if mask is not None:
				mask = mask.unsqueeze(dim=dim)
			if weight is not None:
				weight = weight.unsqueeze(dim=dim)

		return embed, target, mask, weight

	#
	# Cache
	#

	CACHE_MARKER_BYTES = b'6\xa1\x81\xf3c1\xd7~A\x05\xe2p57\x96\x1a\xbasWJ|\xc5\xdb\x8c3Q\xa3\xa9nV\xe8y'
	TARGET_EXCLUDE = {'fixed_token_length'}  # Set of field names in TargetConfig that do not affect cache generation and reuse
	assert not TARGET_EXCLUDE.difference(field.name for field in dataclasses.fields(embedders.TargetConfig))

	def have_cache(self) -> bool:
		return bool(self.cache)

	@contextlib.contextmanager
	def cached(self):
		need_close = not self.cache
		try:
			need_close = self.ensure_cache()
			yield
		finally:
			if need_close:
				self.close_cache()

	def ensure_cache(self) -> bool:
		# Returns whether the cache was newly generated and/or opened
		# The embedder must have a valid target_config when this method is called
		# The embedder inference model is internally temporarily loaded if required

		if not self.use_cache or self.cache:
			return False

		with contextlib.suppress(OSError):
			os.mkdir(self.cache_dir)

		configuration = {
			'embedder': self.embedder.get_configuration(main_config=True, target_config=True, target_exclude=self.TARGET_EXCLUDE),
			'cache_marker_bytes': tuple(self.CACHE_MARKER_BYTES),
			'num_usids': self.num_usids,
			'vocab': self.vocab_json,
			'singular_prompts': tuple(dataclasses.asdict(prompt) for prompt in self.singular_prompts),
			'plural_prompts': tuple(dataclasses.asdict(prompt) for prompt in self.plural_prompts),
			'hypernym_prompts': tuple(dataclasses.asdict(prompt) for prompt in self.hypernym_prompts),
			'no_hypernym_freqs': self.no_hypernym_freqs,
			'make_indefinite': tuple(({key: getattr(regex, key) for key in dir(regex) if key[0] != '_' and not callable(getattr(regex, key))}, article) for regex, article in self.INDEFINITE_REGEXES),
		}

		configuration_hash = hashlib.md5(json.dumps(configuration, separators=(',', ':'), sort_keys=True).encode()).hexdigest()
		cache_path = os.path.join(self.cache_dir, f'noun_dataset_cache_{configuration_hash}.bin')
		log.info(f"Noun dataset cache file: {cache_path}")

		embed_dtype = self.embedder.embed_dtype
		target_dtype = self.embedder.token_dtype
		mask_dtype = self.embedder.target_config.mask_dtype
		embed_dim = self.embedder.embed_dim
		target_dim = self.embedder.target_config.token_length
		mask_dim = target_dim if self.embedder.target_config.use_masks else 0
		embed_dtype_bytes = torch.tensor((), dtype=embed_dtype).element_size()
		target_dtype_bytes = torch.tensor((), dtype=target_dtype).element_size()
		mask_dtype_bytes = torch.tensor((), dtype=mask_dtype).element_size()
		embed_bytes = embed_dim * embed_dtype_bytes
		target_bytes = target_dim * target_dtype_bytes
		mask_bytes = mask_dim * mask_dtype_bytes
		sample_bytes = embed_bytes + target_bytes + mask_bytes

		cache_marker_bytes = self.CACHE_MARKER_BYTES
		num_marker_bytes = len(cache_marker_bytes)
		if num_marker_bytes > sample_bytes:
			num_marker_bytes = sample_bytes
			cache_marker_bytes = cache_marker_bytes[:num_marker_bytes]

		cache_bytes = sample_bytes * self.num_usids
		cache_file_bytes = sample_bytes + cache_bytes
		cache_file_bytes_gib = cache_file_bytes / (1 << 30)
		log.info(f"Noun dataset cache requires {sample_bytes} bytes per USID, leading to {cache_file_bytes_gib:.3f}GiB for {self.num_usids} USIDs including initial marker bytes")

		with self.embedder.inference_model():

			cache_fd = os.open(cache_path, os.O_RDWR | os.O_CREAT)
			try:
				if self.recache or os.read(cache_fd, num_marker_bytes) != cache_marker_bytes or os.fstat(cache_fd).st_size != cache_file_bytes:
					log.info("Generating noun dataset cache file...")
					partial_cache = True
					self.recache = False
					try:
						os.ftruncate(cache_fd, 0)
						os.ftruncate(cache_fd, cache_file_bytes)
						os.lseek(cache_fd, sample_bytes, os.SEEK_SET)
						cache = Cache(file=None, mmap=None, view=None, embed_dtype=embed_dtype, target_dtype=target_dtype, mask_dtype=mask_dtype, embed_dim=embed_dim, target_dim=target_dim, mask_dim=mask_dim, embed_bytes=embed_bytes, target_bytes=target_bytes, mask_bytes=mask_bytes, sample_bytes=sample_bytes)
						generated_cache_bytes = self._generate_cache(cache_fd=cache_fd, cache=cache)
						if generated_cache_bytes != cache_bytes:
							raise ValueError(f"Expected noun dataset cache generation of {cache_bytes} bytes, but got {generated_cache_bytes} bytes instead")
						cache_pos = os.lseek(cache_fd, 0, os.SEEK_CUR)
						if cache_pos != cache_file_bytes:
							raise ValueError(f"Unexpected noun dataset cache file position indicator: {cache_pos}")
						os.lseek(cache_fd, 0, os.SEEK_SET)
						os.pwrite(cache_fd, cache_marker_bytes, 0)
						os.fsync(cache_fd)
						partial_cache = False
						if os.read(cache_fd, num_marker_bytes) != cache_marker_bytes or os.fstat(cache_fd).st_size != cache_file_bytes:
							raise ValueError(f"Noun dataset cache was generated, but something is still wrong: {cache_path}")
					finally:
						if partial_cache:
							with contextlib.suppress(FileNotFoundError):
								os.remove(cache_path)
								log.warning(f"Deleted partial cache file: {cache_path}")
					# noinspection PyUnreachableCode
					log.info("Finished generating noun dataset cache file")
				else:
					log.info("Can reuse existing noun dataset cache file")
			finally:
				os.close(cache_fd)

			with contextlib.ExitStack() as stack:

				cache_file = stack.enter_context(open(cache_path, 'r+b'))
				cache_mmap = stack.enter_context(mmap.mmap(cache_file.fileno(), length=0, flags=mmap.MAP_SHARED, access=mmap.ACCESS_READ))
				cache_mmap.madvise(mmap.MADV_RANDOM)
				cache_mmap.madvise(mmap.MADV_WILLNEED)

				def abort_cache():
					self.cache = None
					nonlocal cache_view
					cache_view.release()

				cache_view = memoryview(cache_mmap).toreadonly()
				self.cache = Cache(file=cache_file, mmap=cache_mmap, view=cache_view, embed_dtype=embed_dtype, target_dtype=target_dtype, mask_dtype=mask_dtype, embed_dim=embed_dim, target_dim=target_dim, mask_dim=mask_dim, embed_bytes=embed_bytes, target_bytes=target_bytes, mask_bytes=mask_bytes, sample_bytes=sample_bytes)
				stack.callback(abort_cache)

				log.info("Sanity checking the noun dataset cache...")
				embed_eps = torch.finfo(self.cache.embed_dtype).eps
				embed_ntol = 4 * embed_eps
				embed_atol = 6 * embed_eps
				if self.embedder.amp_dtype is not None:
					embed_atol = max(embed_atol, torch.finfo(self.embedder.amp_dtype).eps)
				if self.embedder.manual_amp_dtype is not None:
					embed_atol = max(embed_atol, torch.finfo(self.embedder.manual_amp_dtype).eps)
				embed_rtol = 8 * embed_atol
				max_atol = max_rtol = max_ntol = -math.inf
				checked_usids = 0
				expect_mask = self.cache.mask_dim != 0
				get_cached_sample = functools.partial(self.get_cached_sample, use_targets=True)
				usid_iter = map(int, torch.randperm(self.num_usids).numpy()) if self.check_consistent else iter(random.sample(range(self.num_usids), min(self.embedder.inference_batch_size, self.num_usids)))
				with tqdm.tqdm(desc='Checking cache', total=self.num_usids, unit='id', unit_scale=True, dynamic_ncols=True, smoothing=0.08, disable=not self.check_consistent) as progress_bar:
					while usids := tuple(itertools.islice(usid_iter, self.embedder.inference_batch_size)):
						num_usids = len(usids)
						texts, targets = zip(*map(lambda usid: self.get_sample_text(usid=usid), usids))
						with self.embedder.inference_mode():
							embed_tensor = self.embedder.inference_text(text=texts)
						target_tensor, mask_tensor = self.embedder.tokenize_target(text=targets)
						cached_embed_tensors, cached_target_tensors, cached_mask_tensors, _ = zip(*map(get_cached_sample, usids))
						cached_embed_tensor = torch.utils.data.default_collate(cached_embed_tensors)
						cached_target_tensor = torch.utils.data.default_collate(cached_target_tensors)
						cached_mask_tensor = torch.utils.data.default_collate(cached_mask_tensors) if expect_mask else None
						target_tensor_tokens = target_tensor.shape[1]
						cached_target_tensor_tokens = cached_target_tensor.shape[1]
						if not expect_mask == (mask_tensor is not None) == (cached_mask_tensor is not None) == self.embedder.target_config.use_masks:
							raise ValueError("Cache has a mask expectation mismatch")
						if not embed_tensor.dtype == cached_embed_tensor.dtype == self.cache.embed_dtype or not target_tensor.dtype == cached_target_tensor.dtype == self.cache.target_dtype:
							raise ValueError("Cache has an embed/target dtype mismatch")
						if not embed_tensor.shape == cached_embed_tensor.shape == (num_usids, self.cache.embed_dim) or not target_tensor.ndim == cached_target_tensor.ndim == 2 or not target_tensor.shape[0] == cached_target_tensor.shape[0] == num_usids:
							raise ValueError("Cache has an embed/target shape mismatch")
						if target_tensor_tokens > cached_target_tensor_tokens:
							raise ValueError("Cache target tensor is too short")
						if not torch.equal(target_tensor, cached_target_tensor[:, :target_tensor_tokens]):
							raise ValueError("Cache has a target tensor content mismatch")
						if not torch.all(torch.eq(cached_target_tensor[:, target_tensor_tokens:], self.embedder.target_config.pad_token_id)):
							raise ValueError("Cache has a target tensor padding mismatch")
						if expect_mask:
							if not mask_tensor.dtype == cached_mask_tensor.dtype == self.cache.mask_dtype:
								raise ValueError("Cache has a mask dtype mismatch")
							if mask_tensor.shape != target_tensor.shape or cached_mask_tensor.shape != cached_target_tensor.shape:
								raise ValueError("Cache has a mask-target shape mismatch")
							if not torch.equal(mask_tensor, cached_mask_tensor[:, :target_tensor_tokens]):
								raise ValueError("Cache has a mask tensor content mismatch")
							if not torch.all(cached_mask_tensor[:, target_tensor_tokens:]):
								raise ValueError("Cache has a mask tensor padding mismatch")
						if torch.any(cached_target_tensor < 0) or torch.any(cached_target_tensor >= self.embedder.target_config.vocab_size):
							raise ValueError("Cached target tensor has values outside of the vocab range")
						if cached_target_tensor_tokens != self.embedder.target_config.token_length or (self.embedder.target_config.fixed_token_length and target_tensor_tokens != self.embedder.target_config.token_length):
							raise ValueError("Cached target tensor does not have expected token length")
						if self.embedder.target_config.start_token_id is not None and not torch.all(torch.eq(target_tensor[:, 0], self.embedder.target_config.start_token_id)):  # Note: Testing target_tensor is a stronger test than cached_target_tensor
							raise ValueError("Cache has a start token issue")
						cached_target_padding = torch.eq(cached_target_tensor, self.embedder.target_config.pad_token_id)
						if not torch.equal(cached_target_padding, cached_target_padding.cummax(dim=1)[0]):
							raise ValueError("Cache target tensor has non-padding token after padding token")
						if self.embedder.target_config.end_token_id is None:
							if expect_mask and not torch.equal(cached_mask_tensor, torch.eq(cached_target_tensor, self.embedder.target_config.pad_token_id)):
								raise ValueError("Cache mask tensor is not consistent with end-tokenless target tensor")
						else:
							if self.embedder.target_config.end_token_id != self.embedder.target_config.pad_token_id:
								cached_target_end_cummax = torch.eq(cached_target_tensor, self.embedder.target_config.end_token_id).cummax(dim=1)[0]
								if not torch.equal(cached_target_end_cummax[:, :-1], cached_target_padding[:, 1:]) or not torch.all(cached_target_end_cummax[:, -1]) or torch.any(cached_target_padding[:, 0]):
									raise ValueError("Cache has an end token issue")
							if not self.embedder.target_config.fixed_token_length and (not torch.any(torch.eq(target_tensor[:, -1], self.embedder.target_config.end_token_id)) or (target_tensor_tokens >= 2 and torch.all(torch.eq(target_tensor[:, -2:], self.embedder.target_config.pad_token_id)))):
								raise ValueError("Target tensor does not have an end token in the last column despite dynamic length => Is there too much padding?")
							if not torch.all(torch.any(torch.eq(target_tensor, self.embedder.target_config.end_token_id), dim=1)):
								raise ValueError("Target tensor does not have an end token in every row")
						if expect_mask:
							if not torch.equal(cached_mask_tensor, cached_mask_tensor.cummax(dim=1)[0]):
								raise ValueError("Cache mask tensor has non-padding token after padding token")
							if self.embedder.target_config.end_token_id is not None and self.embedder.target_config.end_token_id == self.embedder.target_config.pad_token_id:
								if not torch.equal(cached_mask_tensor[:, 1:], cached_target_padding[:, :-1]) or torch.any(cached_mask_tensor[:, 0]) or not torch.all(cached_target_padding[:, -1]):
									raise ValueError("Cache mask tensor is inconsistent with target tensor padding for equal end/pad tokens")
							elif not torch.equal(cached_mask_tensor, cached_target_padding):
								raise ValueError("Cache mask tensor is inconsistent with target tensor padding for differing end/pad tokens")
						embed_tensor = embed_tensor.cpu()
						embed_error = (embed_tensor - cached_embed_tensor).abs()
						cached_embed_tensor_abs = cached_embed_tensor.abs()
						max_atol = max(max_atol, (embed_error - embed_rtol * cached_embed_tensor_abs).max().item())
						max_rtol = max(max_rtol, ((embed_error - embed_atol) / cached_embed_tensor_abs).max().item())
						if not torch.allclose(embed_tensor, cached_embed_tensor, rtol=embed_rtol, atol=embed_atol):
							raise ValueError(f"Cache has an embedding vector mismatch (allowed atol {embed_atol:.3g} vs seen atol {max_atol:.3g}, allowed rtol {embed_rtol:.3g} vs seen rtol {max_rtol:.3g})")
						embed_norm_ones = embed_tensor.new_ones(size=(num_usids,))
						embed_tensor_norm = torch.linalg.vector_norm(embed_tensor, dim=1)
						cached_embed_tensor_norm = torch.linalg.vector_norm(cached_embed_tensor, dim=1)
						max_ntol = max(max_ntol, max((embed_tensor_norm - 1).abs().max().item(), (cached_embed_tensor_norm - 1).abs().max().item()))
						if not torch.allclose(embed_tensor_norm, embed_norm_ones, atol=embed_ntol, rtol=0) or not torch.allclose(cached_embed_tensor_norm, embed_norm_ones, atol=embed_atol, rtol=0):
							raise ValueError(f"Cache has an embedding unnormality (allowed ntol {embed_ntol:.3g} vs seen ntol {max_ntol:.3g})")
						progress_bar.update(num_usids)
						checked_usids += num_usids
				log.info(f"Noun dataset cache passed a sanity check of {checked_usids} USIDs (allowed atol {embed_atol:.3g} vs seen atol {max_atol:.3g}, allowed rtol {embed_rtol:.3g} vs seen rtol {max_rtol:.3g}, allowed ntol {embed_ntol:.3g} vs seen ntol {max_ntol:.3g})")

				stack.pop_all()

		return True

	def _generate_cache(self, cache_fd: int, cache: Cache) -> int:
		generated_cache_bytes = 0
		assert cache.mask_dim == (cache.target_dim if self.embedder.target_config.use_masks else 0)
		target_padding_view = memoryview(torch.full(size=(cache.target_dim,), fill_value=self.embedder.target_config.pad_token_id, dtype=cache.target_dtype).numpy())
		mask_padding_view = memoryview(torch.ones(size=(cache.mask_dim,), dtype=cache.mask_dtype).numpy()) if cache.mask_dim != 0 else None
		with self.embedder.inference_mode(), contextlib.ExitStack() as stack:
			index = 0
			total_samples = 0
			it = iter(self.unique_sample())
			while unique_samples := tuple(itertools.islice(it, self.embedder.inference_batch_size)):
				num_samples = len(unique_samples)
				embed_tensor = self.embedder.inference_text(text=tuple(unique_sample.text for unique_sample in unique_samples))
				target_tensor, mask_tensor = self.embedder.tokenize_target(text=tuple(unique_sample.target for unique_sample in unique_samples))
				assert (mask_tensor is None) == (cache.mask_dim == 0)
				if embed_tensor.dtype != cache.embed_dtype or embed_tensor.shape != (num_samples, cache.embed_dim):
					raise ValueError(f"Unexpected embedding tensor: Shape {tuple(embed_tensor.shape)}, DType {embed_tensor.dtype}")
				if target_tensor.dtype != cache.target_dtype or target_tensor.ndim != 2 or target_tensor.shape[0] != num_samples:
					raise ValueError(f"Unexpected target tensor: Shape {tuple(target_tensor.shape)}, DType {target_tensor.dtype}")
				if mask_tensor is not None and (mask_tensor.dtype != cache.mask_dtype or mask_tensor.shape != target_tensor.shape):
					raise ValueError(f"Unexpected mask tensor: Shape {tuple(mask_tensor.shape)}, DType {mask_tensor.dtype}")
				num_padding = cache.target_dim - target_tensor.shape[1]
				if num_padding < 0:
					raise ValueError(f"Target noun tensor is too large ({target_tensor.shape[1]}) for the configured cache target dimension ({cache.target_dim})")
				elif num_padding > 0:
					target_padding_subview = target_padding_view[:num_padding]
					mask_padding_subview = mask_padding_view[:num_padding] if mask_padding_view is not None else None
				target_view = memoryview(target_tensor.numpy())
				mask_view = memoryview(mask_tensor.numpy()) if mask_tensor is not None else None
				embed_view = memoryview(embed_tensor.cpu().numpy())
				for i in range(num_samples):
					embed_bytes = os.write(cache_fd, embed_view[i:i+1])  # noqa
					if embed_bytes != cache.embed_bytes:
						raise ValueError(f"Wrote unexpected number of embedding tensor bytes: {embed_bytes} vs {cache.embed_bytes}")
					target_bytes = os.write(cache_fd, target_view[i:i+1])  # noqa
					if num_padding > 0:
						target_bytes += os.write(cache_fd, target_padding_subview)  # noqa
					if target_bytes != cache.target_bytes:
						raise ValueError(f"Wrote unexpected number of target tensor bytes: {target_bytes} vs {cache.target_bytes}")
					generated_cache_bytes += embed_bytes + target_bytes
					if mask_view is not None:
						mask_bytes = os.write(cache_fd, mask_view[i:i+1])  # noqa
						if num_padding > 0:
							mask_bytes += os.write(cache_fd, mask_padding_subview)  # noqa
						if mask_bytes != cache.mask_bytes:
							raise ValueError(f"Wrote unexpected number of mask tensor bytes: {mask_bytes} vs {cache.mask_bytes}")
						generated_cache_bytes += mask_bytes
				total_samples += num_samples
				if index == 1:
					progress_bar = stack.enter_context(tqdm.tqdm(desc='Generating cache', total=self.num_usids, unit='id', unit_scale=True, dynamic_ncols=True, smoothing=0.08, initial=total_samples))
				elif index > 1:
					progress_bar.set_postfix_str(f'{generated_cache_bytes / 1048576:.0f}MiB', refresh=False)
					progress_bar.update(num_samples)
				index += 1
			assert total_samples == self.num_usids
		return generated_cache_bytes

	def get_cached_sample(self, usid, use_targets: bool = None) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor], None]:

		if not self.cache:
			raise RuntimeError("Cache is required but was not prepared")
		if usid < 0 or usid >= self.num_usids:
			raise IndexError("USID out of range")

		cache_offset = self.cache.sample_bytes * (usid + 1)
		embed_tensor = torch.frombuffer(self.cache.view, dtype=self.cache.embed_dtype, count=self.cache.embed_dim, offset=cache_offset)

		if self.use_targets if use_targets is None else use_targets:
			cache_offset += self.cache.embed_bytes
			target_tensor = torch.frombuffer(self.cache.view, dtype=self.cache.target_dtype, count=self.cache.target_dim, offset=cache_offset)
			if self.cache.mask_dim != 0:
				cache_offset += self.cache.target_bytes
				mask_tensor = torch.frombuffer(self.cache.view, dtype=self.cache.mask_dtype, count=self.cache.mask_dim, offset=cache_offset)
			else:
				mask_tensor = None
		else:
			target_tensor = None
			mask_tensor = None

		return embed_tensor, target_tensor, mask_tensor, None

	def close_cache(self):
		if self.cache:
			try:
				self.cache.view.release()
			finally:
				try:
					self.cache.mmap.close()
				finally:
					try:
						self.cache.file.close()
					finally:
						self.cache = None

	#
	# Miscellaneous
	#

	INDEFINITE_REGEXES = (
		(re.compile(r"^[AEFHILMNORSX][B-Z][A-Z]?\b"), 'an '),
		(re.compile(r"^([aefhilmnorsx][.-]|hour)", re.IGNORECASE), 'an '),
		(re.compile(r"^([a-z][.-]|e[uw]|onc?e\b|uni|u[bcfghjkqrst][aeiou])", re.IGNORECASE), 'a '),
		(re.compile(r"^[aeiou]", re.IGNORECASE), 'an '),
	)

	@classmethod
	def make_indefinite(cls, noun, suffix=None) -> str:
		# Note: This is orders of magnitude faster than the inflect library (at the cost of rare inaccuracies, but at the benefit of rare corrections)
		# Note: This method was specifically optimised in the context of the full vocabulary file (e.g. was used to prune regexes that don't actually change anything in our noun case)
		for regexen, article in cls.INDEFINITE_REGEXES:
			if regexen.match(noun):
				break
		else:
			article = 'a '
		return f"{article}{noun}{suffix}" if suffix is not None else article + noun
# EOF
